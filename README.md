# üéôÔ∏èVoice Based Pronunciation Prediction üé§

Welcome to the **Voice Based Pronunciation Prediction** project! This tool helps users analyze pronunciation quality through audio recordings, using powerful Automatic Speech Recognition (ASR) models like Jasper, QuartzNet, Wav2Vec2, Hubert, and Whisper. It evaluates recorded audio against a provided transcription, visualizes correct and incorrect pronunciations, and provides text-to-speech feedback with pronunciation improvement tips.

---

## üìã Contents
- [Introduction](#introduction)
- [Features](#features)
- [Installation](#installation)
- [Usage Guide](#usage-guide)
- [Model Documentation](#model-documentation)
- [Visualization & Feedback](#visualization--feedback)
- [Contributing](#contributing)

---

## üìñ Introduction

Feedback on pronunciation can be invaluable in spoken language learning or accent refinement. This project allows users to record audio and receive an objective evaluation of pronunciation accuracy based on various metrics. By using five ASR models, this tool compares recorded audio with a ground-truth transcription, calculating error rates and displaying visual feedback. It‚Äôs a perfect tool for learners, teachers, and researchers interested in audio analysis and language evaluation.

---

## ‚ú® Features

### üéß Multi-Model Transcription
Supports transcription with:
- **Jasper** - A robust NVIDIA model well-suited for noisy environments.
- **QuartzNet** - Another NVIDIA model, designed to be lightweight and fast.
- **Wav2Vec2** - A model from Facebook AI with high accuracy on standard datasets.
- **Hubert** - Known for high performance in speech representation learning.
- **Whisper** - OpenAI‚Äôs versatile ASR model with multilingual support.

### üìä Error Metrics
Calculates three key metrics:
- **Word Error Rate (WER)**: Measures how many words differ between ground truth and predictions.
- **Match Error Rate (MER)**: Compares the sequence alignment of words.
- **Word Information Loss (WIL)**: A metric for how much information is lost.

### üñºÔ∏è Pronunciation Visualization
Displays each word in the transcription in green or red, based on whether it was correctly or incorrectly pronounced, making it easy to identify pronunciation strengths and areas for improvement.

### üó£Ô∏è Text-to-Speech Feedback
Offers spoken feedback based on error rates, encouraging users with tips to improve pronunciation clarity.

---

## ‚öôÔ∏è Installation

To set up this project on your local machine, follow these steps:

1. **Clone the Repository**
    ```bash
   git clone https://github.com/Jonam-2004/voice-based-pronunciation-prediction.git
   cd voice-based-pronunciation-prediction
    ```

2. **Install Dependencies**
    Ensure you have Python 3.7+ installed. Install the required packages with:
    ```bash
    pip install -r requirements.txt
    ```

---

## üöÄ Usage Guide

1. **Run the Script**
   Start the evaluation script to record audio, transcribe, and visualize results using notebook files.

2. **Record Audio**
   The script will record audio for a specified duration (default: 5 seconds). Speak clearly into your microphone.

3. **Provide Ground Truth Transcription**
   After recording, you can just enter the text you were reading or speaking. This text serves as a reference for evaluating pronunciation accuracy.

4. **View Evaluation Results**
   - **Error Metrics Plots**: Visualize WER, MER, and WIL for each model.
   - **Pronunciation Visualization**: Green for correct, red for mispronounced words.
   - **Text-to-Speech Tips**: Hear feedback based on the model with the highest error rate.

---

## üìö Model Documentation

Each model contributes uniquely to the evaluation process. For further insights into their architectures and capabilities, refer to the official documentation links below:

| Model Name   | Documentation Link                                           |
|--------------|--------------------------------------------------------------|
| **Jasper**   | [Jasper Model Documentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_jasper10x5dr) |
| **QuartzNet**| [QuartzNet Model Documentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_quartznet15x5) |
| **Wav2Vec2** | [Wav2Vec2 Documentation](https://huggingface.co/docs/transformers/model_doc/wav2vec2) |
| **Hubert**   | [Hubert Documentation](https://huggingface.co/docs/transformers/model_doc/hubert) |
| **Whisper**  | [Whisper Documentation](https://github.com/openai/whisper) |

---

## üìä Visualization & Feedback

### Error Metrics Visualization
Bar charts display error rates for each model, helping users compare model performance. Metrics include:
- **Word Error Rate (WER)**
- **Match Error Rate (MER)**
- **Word Information Loss (WIL)**

### Pronunciation Visualization
A unique color-coded display shows:
- **Green**: Words pronounced correctly.
- **Red**: Mispronounced words.

This visualization, along with metric scores, allows users to quickly identify pronunciation accuracy and areas needing improvement.

### Text-to-Speech Feedback
Based on the highest error rate among models, TTS feedback provides tailored tips:
- **High Error (>0.5)**: "Try to speak more clearly and pronounce each word distinctly."
- **Moderate Error (>0.3)**: "Focus on enunciating words carefully to reduce misinterpretation."
- **Low Error (‚â§0.3)**: "Good pronunciation! Keep practicing for even better clarity."

---

## ü§ù Contributing

We welcome contributions! To contribute:
1. **Fork** the repository.
2. **Create a branch** for your feature or bug fix.
3. **Submit a pull request** for review.


---

## Authors
- [Manoj S](https://github.com/Jonam-2004)
- [Mohamed Aslam K](https://github.com/Mohamedaslam227)
  
## üìß Contact

For questions, suggestions, or feedback, feel free to reach out at [51manoj2004@gmail.com](mailto:51manoj2004@gmail.com), [mohamedaslam2254@gmail.com](mailto:mohamedaslam2254@gmail.com).

---

Thank you for exploring this project! Happy pronouncing! üéâ
